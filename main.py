# Class for actor network
# Class for critic network
# Class for action noise
# Class for replay buffer
# Class for agent functionality
    # Memory, actor/critic networks, target nets, tau

# Initialize actor & critic networks ğ›(s|ğ›‰^ğ›) Q(s, a|ğ›‰^Q)

# Initialize target networks with weights from main networks

# Initialize replay buffer R

# For a large number of episodes
    # Initialize noise process N for action exploration
    # Reset environment
    # For each step of game
        # at = ğ›(st|ğ›‰^ğ›) + Nt
        # Take action at and get new state, reward
        # Store (st, at, rt, St+1, donet) in R
        # Sample random minibatch of (si, ai, ri, si+1) from R
        # yi = ri + rQ'(si+1, ğ›'(si+1|ğ›‰^ğ›')|ğ›‰^Q)
        # Update critic by minimizing :=1/Nğšº(yi-Q(si, ai|ğ›‰^Q))^2
        # Update actor ğ›ğ›‰â‰ˆ1/Nğšºğ›ğ›‰Q(s, a|ğ›‰^Q)|s=s, a=ğ›(st|ğ›‰^ğ›)
        # ğ›‰^Q <- ğ›•ğ›‰^Q+(1-ğ›„)ğ›‰^Q
        # ğ›‰^ğ› <- ğ›•ğ›‰^ğ› + (1-ğ›„)ğ›‰^ğ›‰^ğ›